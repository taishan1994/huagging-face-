# 把它们放在一起

在最后几节中，我们一直在尽最大努力手动完成大部分工作。 我们已经探索了分词器的工作原理，并研究了分词、转换为输入 ID、填充、截断和注意力掩码。 

然而，正如我们在第 2 节中看到的，🤗 Transformers API 可以通过我们将在这里深入研究的高级函数为我们处理所有这些。 当你直接在句子上调用你的分词器时，你会得到准备通过你的模型的输入：

 ```python
 from transformers import AutoTokenizer
 
 checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
 tokenizer = AutoTokenizer.from_pretrained(checkpoint)
 
 sequence = "I've been waiting for a HuggingFace course my whole life."
 
 model_inputs = tokenizer(sequence)
 ```

在这里，model_inputs 变量包含模型正常运行所需的一切。 对于 DistilBERT，这包括输入 ID 和注意力掩码。 接受额外输入的其他模型也将具有分词器对象的输出。 

正如我们将在下面的一些示例中看到的，这种方法非常强大。 首先，它可以标记单个序列：

```python
sequence = "I've been waiting for a HuggingFace course my whole life."

model_inputs = tokenizer(sequence)
```

它一次也处理多个序列，API没有更改：

```python
sequences = [
  "I've been waiting for a HuggingFace course my whole life.",
  "So have I!"
]

model_inputs = tokenizer(sequences)
```

它可以根据若干目标填充：

```python
# Will pad the sequences up to the maximum sequence length
model_inputs = tokenizer(sequences, padding="longest")

# Will pad the sequences up to the model max length
# (512 for BERT or DistilBERT)
model_inputs = tokenizer(sequences, padding="max_length")

# Will pad the sequences up to the specified max length
model_inputs = tokenizer(sequences, padding="max_length", max_length=8)
```

它也可以截断句子：

```python
sequences = [
  "I've been waiting for a HuggingFace course my whole life.",
  "So have I!"
]

# Will truncate the sequences that are longer than the model max length
# (512 for BERT or DistilBERT)
model_inputs = tokenizer(sequences, truncation=True)

# Will truncate the sequences that are longer than the specified max length
model_inputs = tokenizer(sequences, max_length=8, truncation=True)
```

tokenizer 对象可以处理到特定框架张量的转换，然后可以将其直接发送到模型。 例如，在以下代码示例中，我们提示分词器返回来自不同框架的张量——“pt”返回 PyTorch 张量，“tf”返回 TensorFlow 张量，“np”返回 NumPy 数组： 

```python
sequences = [
  "I've been waiting for a HuggingFace course my whole life.",
  "So have I!"
]

# Returns PyTorch tensors
model_inputs = tokenizer(sequences, padding=True, return_tensors="pt")

# Returns TensorFlow tensors
model_inputs = tokenizer(sequences, padding=True, return_tensors="tf")

# Returns NumPy arrays
model_inputs = tokenizer(sequences, padding=True, return_tensors="np")
```

# 特殊标记

如果我们查看分词器返回的输入 ID，我们会发现它们与我们之前的有一点不同： 

```python
sequence = "I've been waiting for a HuggingFace course my whole life."

model_inputs = tokenizer(sequence)
print(model_inputs["input_ids"])

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
print(ids)
[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102]
[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012]
```

开头添加了一个令牌 ID，结尾添加了一个。 让我们解码上面的两个 ID 序列，看看这是关于什么的： 

```python
print(tokenizer.decode(model_inputs["input_ids"]))
print(tokenizer.decode(ids))
"[CLS] i've been waiting for a huggingface course my whole life. [SEP]"
"i've been waiting for a huggingface course my whole life."
```

分词器在开头添加了特殊词 [CLS]，在末尾添加了特殊词 [SEP]。 这是因为模型是用这些进行预训练的，所以为了获得相同的推理结果，我们还需要添加它们。 请注意，有些模型不添加特殊词，或添加不同的词； 模型也可以仅在开头或仅在结尾添加这些特殊词。 在任何情况下，分词器都知道哪些是预期的，并将为您处理。 

# 总结：从分词器到模型 

现在我们已经看到了分词器对象在应用于文本时使用的所有单独步骤，让我们最后一次看看它如何处理多个序列（填充！）、非常长的序列（截断！）以及多种类型的张量及其 主要API： 

```python
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
  "I've been waiting for a HuggingFace course my whole life.",
  "So have I!"
]

tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors="pt")
output = model(**tokens)
```

