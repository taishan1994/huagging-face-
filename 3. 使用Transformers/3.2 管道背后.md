# 管道背后

先从一个完整的例子开始，看看幕后发生了什么：

```python
from transformers import pipeline

classifier = pipeline("sentiment-analysis")
classifier([
    "I've been waiting for a HuggingFace course my whole life.", 
    "I hate this so much!",
])
```

输出：

```python
[{'label': 'POSITIVE', 'score': 0.9598047137260437},
 {'label': 'NEGATIVE', 'score': 0.9994558095932007}]
```

该管道将三个步骤组合在一起：预处理、通过模型传递输入和后处理：

![The full NLP pipeline: tokenization of text, conversion to IDs, and inference through the Transformer model and the model head.](https://huggingface.co/course/static/chapter2/full_nlp_pipeline.png)

让我们快速浏览一下这些内容。

## 使用分词器进行预处理

与其他神经网络一样，Transformer 模型不能直接处理原始文本，因此我们管道的第一步是将文本输入转换为模型可以理解的数字。 为此，我们使用了一个分词器，它将负责：

- 将输入拆分为称为分词的单词、子词或符号（如标点符号）
- 将每个分词映射到一个整数
- 添加可能对模型有用的其他输入

所有这些预处理都需要以与模型预训练时完全相同的方式完成，因此我们首先需要从[Model Hub](https://huggingface.co/models)下载该信息。 为此，我们使用 AutoTokenizer 类及其 from_pretrained 方法。 使用我们模型的检查点名称，它会自动获取与模型的分词器关联的数据并缓存它（所以它只在你第一次运行下面的代码时下载）。

由于情感分析管道的默认检查点是 distilbert-base-uncased-finetuned-sst-2-english（您可以在[here](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)查看其模型卡），我们运行以下命令：

```PYTHON
from transformers import AutoTokenizer

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
```

一旦我们有了分词器，我们就可以直接将我们的句子传递给它，我们将得到一个准备好提供给我们模型的字典！ 唯一要做的就是将输入 ID 列表转换为张量。

您可以使用 🤗 Transformers，而不必担心使用哪个 ML 框架作为后端； 对于某些模型，它可能是 PyTorch 或 TensorFlow，或 Flax。 然而，Transformer 模型只接受张量作为输入。 如果这是您第一次听说张量，您可以将它们视为 NumPy 数组。 NumPy 数组可以是标量 (0D)、向量 (1D)、矩阵 (2D) 或具有更多维度。 它实际上是一个张量； 其他 ML 框架的张量行为类似，并且通常与 NumPy 数组一样易于实例化。

要指定我们想要返回的张量类型（PyTorch、TensorFlow 或普通 NumPy），我们使用 return_tensors 参数：

```PYTHON
raw_inputs = [
    "I've been waiting for a HuggingFace course my whole life.", 
    "I hate this so much!",
]
inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors="pt")
print(inputs)
```

时不要担心填充和截断； 我们稍后会解释这些。 这里要记住的主要事情是您可以传递一个句子或一个句子列表，以及指定要返回的张量类型（如果没有传递类型，您将获得一个包含列表的列表作为结果） .

以下是 PyTorch 张量的结果：

```python
{
    'input_ids': tensor([
        [  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172, 2607,  2026,  2878,  2166,  1012,   102],
        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,     0,     0,     0,     0,     0,     0]
    ]), 
    'attention_mask': tensor([
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
    ])
}
```

输出本身是一个包含两个键的字典，input_ids 和 attention_mask。 input_ids 包含两行整数（每个句子一个），它们是每个句子中分词的唯一标识符。 我们将在本章后面解释什么是 attention_mask。

# 浏览模型

我们可以像使用分词器一样下载我们的预训练模型。 🤗 Transformers 提供了一个 AutoModel 类，它也有一个 from_pretrained 方法：

```python
from transformers import AutoModel

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = AutoModel.from_pretrained(checkpoint)
```

在此代码片段中，我们下载了之前在管道中使用的相同检查点（它实际上应该已经被缓存）并用它实例化了一个模型。

这个架构只包含基本的 Transformer 模块：给定一些输入，它输出我们称之为隐藏状态的东西，也称为特征。 对于每个模型输入，我们将检索一个高维向量，表示 Transformer 模型对该输入的上下文理解。

如果你还没有理解，请不要担心。 我们稍后会解释这一切。

虽然这些隐藏状态本身就很有用，但它们通常是模型另一部分（称为头部）的输入。 在第 2 章中，可以使用相同的体系结构执行不同的任务，但是这些任务中的每一个都有与之关联的不同头。

# 什么是一个高维的向量？

Transformer 模块输出的向量通常很大。 它一般具有三个维度：

- 批量大小：一次处理的序列数（在我们的示例中为 2）。
- 序列长度：序列的数字表示的长度（在我们的示例中为 16）。
- 隐藏大小：每个模型输入的向量维度。

最后一个值，它被称为“高维”。 隐藏大小可能非常大（768 对较小的模型很常见，在较大的模型中可以达到 3072 或更多）。

如果我们将预处理过的输入提供给我们的模型，我们可以看到这一点：

```python
outputs = model(**inputs)
print(outputs.last_hidden_state.shape)
```

输出：

```python
torch.Size([2, 16, 768])
```

请注意，🤗 Transformers 模型的输出行为类似于命名元组或字典。 您可以通过属性（就像我们所做的那样）或键（outpus[“last_hidden_state”]），或者如果您确切知道要查找的内容（outputs[0]）的位置，甚至可以通过索引访问元素。

# 模型头：理解数字

模型头将隐藏状态的高维向量作为输入，并将它们投影到不同的维度上。 它们通常由一个或几个线性层组成：

![A Transformer network alongside its head.](https://huggingface.co/course/static/chapter2/transformer_and_head.png)

Transformer 模型的输出直接送到模型头部进行处理。

在此图中，模型由其嵌入层和后续层表示。 嵌入层将分词化输入中的每个输入 ID 转换为表示关联分词的向量。 随后的层使用注意力机制操纵这些向量以产生句子的最终表示。

🤗 Transformers 中有许多不同的架构可用，每一种架构都围绕着处理特定任务而设计。 以下是一份非详尽清单：

- `*Model` (retrieve the hidden states)
- `*ForCausalLM`
- `*ForMaskedLM`
- `*ForMultipleChoice`
- `*ForQuestionAnswering`
- `*ForSequenceClassification`
- `*ForTokenClassification`
- and others 🤗

对于我们的示例，我们需要一个带有序列分类头的模型（能够将句子分类为正面或负面）。 因此，我们实际上不会使用 AutoModel 类，而是使用 AutoModelForSequenceClassification：

```python
from transformers import AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
outputs = model(**inputs)
```

现在，如果我们查看输入的形状，维度会低得多：模型头将我们之前看到的高维向量作为输入，并输出包含两个值（每个标签一个）的向量：

```python
print(outputs.logits.shape)
torch.Size([2, 2])
```

因为我们只有两个句子和两个标签，所以我们从模型中得到的结果是 2 x 2。

# 后处理输出

我们从模型中获得的作为输出的值本身并不一定有意义。让我们看一看：

```python
print(outputs.logits)
tensor([[-1.5607,  1.6123],
        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward>)
```

我们的模型预测了第一个句子为 [-1.5607, 1.6123] 和第二个句子的为[4.1692, -3.3464]。 这些不是概率，而是 logits，即模型最后一层输出的原始非标准化分数。 要转换为概率，它们需要经过一个 SoftMax 层（所有🤗 Transformers 模型都输出 logits，因为训练一般会将最后的激活函数（如 SoftMax）与实际的损失函数（如cross entropy）融合：

```python
import torch

predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
print(predictions)
tensor([[4.0195e-02, 9.5980e-01],
        [9.9946e-01, 5.4418e-04]], grad_fn=<SoftmaxBackward>)
```

现在我们可以看到模型预测了第一个句子的 [0.0402, 0.9598] 和第二个句子的 [0.9995, 0.0005]。 这些是可识别的概率分数。

要获取与每个位置对应的标签，我们可以检查模型配置的 id2label 属性（下一节将详细介绍）：

```py
model.config.id2label
{0: 'NEGATIVE', 1: 'POSITIVE'}
```

现在我们可以得出结论，该模型预测了以下内容：

- 第一句：负：0.0402，正：0.9598
- 第二句：负：0.9995，正：0.0005

我们已经成功地重现了管道的三个步骤：使用分词器进行预处理、通过模型传递输入以及后处理！ 现在让我们花点时间深入了解每个步骤。