# 处理多个序列

在上一节中，我们探讨了最简单的用例：对单个小长度序列进行推理。 然而，一些问题已经出现： 

- 我们如何处理多个序列？ 
- 我们如何处理不同长度的多个序列？ 
- 词汇索引是唯一能让模型正常工作的输入吗？ 
- 有没有序列太长这样的事情？ 

让我们看看这些问题会带来什么样的问题，以及我们如何使用 🤗 Transformers API 解决它们。 

# 使用一批数据输入到模型

在上一项练习中，您了解序列如何转化为数字列表。 让我们将此数字列表转换为Tensor并将其发送到模型：

```python
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
input_ids = torch.tensor(ids)
# This line will fail.
model(input_ids)
```

结果：

```python
IndexError: Dimension out of range (expected to be in range of [-1, 0], but got 1)
```

不好了！ 为什么这失败了？

问题是我们向模型发送了单个序列，而 🤗 Transformers 模型默认需要多个句子。 在这里，当我们将分词器应用于一个序列时，我们尝试在幕后完成它所做的一切，但是如果您仔细观察，您会发现它不仅将输入 ID 列表转换为张量，还添加了一个维度 在它的上面： 

```python
tokenized_inputs = tokenizer(sequence, return_tensors="pt")
print(tokenized_inputs["input_ids"])
tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,
          2607,  2026,  2878,  2166,  1012,   102]])
```

让我们再试一次并添加新的维度：

```python
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)

input_ids = torch.tensor([ids])
print("Input IDs:", input_ids)

output = model(input_ids)
print("Logits:", output.logits)
```

我们打印输入ID以及生成的Logits - 这是输出：

```python
Input IDs: [[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607, 2026,  2878,  2166,  1012]]
Logits: [[-2.7276,  2.8789]]
```

批处理是通过模型同时发送多个句子的行为。 如果您只有一句话，您只需用单个序列构建批处理：

```python
batched_ids = [ids, ids]
```

这是含有两种相同的序列的批次！

# 填充输入

以下包含列表的列表无法转换为Tensor：

```python
batched_ids = [
  [200, 200, 200],
  [200, 200]
]
```

为了解决这个问题，我们将使用填充使我们的张量具有矩形形状。 填充通过向具有较少值的句子添加一个称为填充标记的特殊词来确保我们所有的句子具有相同的长度。 例如，如果您有 10 个包含 10 个单词的句子和 1 个包含 20 个单词的句子，填充将确保所有句子都有 20 个单词。 在我们的示例中，生成的张量如下所示： 

```python
padding_id = 100

batched_ids = [
  [200, 200, 200],
  [200, 200, padding_id]
]
```

填充令牌ID可以在Tokenizer.Pad_Token_ID中找到。 让我们使用它，通过模型分别发送两句话，并将它们批处理在一起：

```python
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence1_ids = [[200, 200, 200]]
sequence2_ids = [[200, 200]]
batched_ids = [[200, 200, 200], [200, 200, tokenizer.pad_token_id]]

print(model(torch.tensor(sequence1_ids)).logits)
print(model(torch.tensor(sequence2_ids)).logits)
print(model(torch.tensor(batched_ids)).logits)
tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward>)
tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward>)
tensor([[ 1.5694, -1.3895],
        [ 1.3373, -1.2163]], grad_fn=<AddmmBackward>)
```

我们成批预测中的logits有问题：第二行应该与第二句的logits相同，但是我们得到了完全不同的值！

这是因为 Transformer 模型的关键特征是将每个标记语境化的注意力层。 这些将考虑填充令牌，因为它们涉及序列的所有令牌。 为了在通过模型传递不同长度的单个句子或传递具有相同句子和填充的批次时获得相同的结果，我们需要告诉这些注意层忽略填充标记。 这是通过使用注意力掩码来完成的。 

# 注意力掩码

attention mask 是形状与输入 IDs tensor 完全相同的张量，填充 0s 和 1s：1s 表示应该关注相应的标记，0s 表示不应关注相应的标记（即它们应该被模型的注意力层忽略 ）。 

让我们用一个注意力掩码来完成前面的例子： 

```python
batched_ids = [
    [200, 200, 200],
    [200, 200, tokenizer.pad_token_id]
]

attention_mask = [
  [1, 1, 1],
  [1, 1, 0]
]
outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))
print(outputs.logits)
tensor([[ 1.5694, -1.3895],
        [ 0.5803, -0.4125]], grad_fn=<AddmmBackward>)
```

现在，我们为批处理中的第二个句子获得了相同的 logits。 

注意第二个序列的最后一个值是一个填充 ID，它在注意力掩码中是一个 0 值。 

# 长序列

对于 Transformer 模型，我们可以传递模型的序列长度是有限制的。 大多数模型处理多达 512 或 1024 个标记的序列，并且在被要求处理更长的序列时会崩溃。 

这个问题有两种解决方案： 

- 使用支持更长序列长度的模型。 
- 截断你的序列。 

模型支持不同的序列长度，有些模型专门处理很长的序列。 [Longformer](https://huggingface.co/transformers/model_doc/longformer.html) 是一个例子，另一个是 [LED](https://huggingface.co/transformers/model_doc/led.html).。 如果您正在处理需要很长序列的任务，我们建议您查看这些模型。 否则，我们建议您通过指定 max_sequence_length 参数来截断序列： 

```python
sequence = sequence[:max_sequence_length]
```

