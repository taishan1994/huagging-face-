# 章末测验 

1. 语言建模流水线的顺序是什么？ 

- 首先是模型，它处理文本并返回原始预测。然后分词器理解这些预测，并在需要时将它们转换回文本。 
- 首先是分词器，它处理文本并返回 ID。该模型处理这些 ID 并输出一个预测，它可以是一些文本。 
- **分词器处理文本并返回 ID。该模型处理这些 ID 并输出预测。然后可以再次使用分词器将这些预测转换回某些文本。**

2. 基础Transformer模型输出的张量有多少维，它们是什么？ 

- 序列长度和批量大小 
- 序列长度和隐藏大小 
- **序列长度、批量大小和隐藏大小** 

3. 以下哪个是子词分词的例子？ 

- **WordPiece** 
- Character-based 
- tokenization 
- Splitting on whitespace and punctuation
- **BPE** 
- **Unigram** 
- None of the above

4. 什么是模型头？ 

- 基础 Transformer 网络的一个组件，可将张量重定向到正确的层 
- 也称为自注意机制，它根据序列的其他标记来调整一个标记的表示 
- **一个附加组件，通常由一层或几层组成，用于将 Transformer 预测转换为特定于任务的输出**

5. 什么是 AutoModel？ 

- 自动训练数据的模型 
- **基于检查点返回正确架构的对象** 
- 自动检测用于其输入的语言以加载正确权重的模型 

6. 将不同长度的序列一起批处理时需要注意哪些技术？ 

- **截断** 
- 返回张量 
- **填充** 
- **注意力掩蔽** 

7. 将 SoftMax 函数应用于序列分类模型输出的 logits 有什么意义？

- 它软化了 logits，使它们更可靠。 
- **它应用了下限和上限，以便它们可以理解。**
- **输出的总和为 1，导致可能的概率解释。**

8. 大多数分词器 API 都围绕什么方法？ 

- 编码，因为它可以将文本编码为 ID，并将 ID 编码为预测 
- 直接调用分词器对象。 
- 填充 
- 标记化 

> 分词器的 __call__ 方法是一个非常强大的方法，它几乎可以处理任何事情。 它也是用于从模型中检索预测的方法。 

9. 这个代码示例中的结果变量包含什么？ 

```py
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
result = tokenizer.tokenize("Hello!")
```

- **字符串列表，每个字符串都是一个标记** 
- ID 列表 
- 包含所有标记的字符串 

10. 下面的代码有问题吗？ 

```python
from transformers import AutoTokenizer, AutoModel

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
model = AutoModel.from_pretrained("gpt2")

encoded = tokenizer("Hey!", return_tensors="pt")
result = model(**encoded)
```

- 不，这似乎是正确的。 
- **分词器和模型应该始终来自同一个检查点。** 
- 使用分词器填充和截断是一种很好的做法，因为每个输入都是一个批次。 
