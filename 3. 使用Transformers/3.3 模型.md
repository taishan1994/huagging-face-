# 模型

在本节中，我们将仔细研究创建和使用模型。 我们将使用 AutoModel 类，当您想从检查点实例化任何模型时，它非常方便。

AutoModel 类及其所有相关类实际上是库中各种可用模型的简单包装器。 这是一个聪明的包装器，因为它可以自动为您的检查点猜测合适的模型架构，然后使用该架构实例化模型。

但是，如果您知道要使用的模型类型，则可以直接使用定义其架构的类。 让我们来看看它如何与 BERT 模型配合使用。

# 创建一个Transformer

初始化 BERT 模型需要做的第一件事是加载配置对象：

```python
from transformers import BertConfig, BertModel

# Building the config
config = BertConfig()

# Building the model from the config
model = BertModel(config)
```

配置包含许多用于构建模型的属性：

```python
print(config)
BertConfig {
  [...]
  "hidden_size": 768,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  [...]
}
```

虽然您还没有看到所有这些属性的作用，但您应该认识其中的一些： hidden_size 属性定义了 hidden_states 向量的大小，而 num_hidden_layers 定义了 Transformer 模型的层数。

# 不同的加载方法

从默认配置创建模型会使用随机值对其进行初始化：

```python
from transformers import BertConfig, BertModel

config = BertConfig()
model = BertModel(config)
```

模型可以在这种状态下使用，但是会输出乱码； 它需要先训练。 我们可以根据手头的任务从头开始训练模型，但是正如您在第 1 章中看到的那样，这将需要很长时间和大量数据，并且会对环境产生不可忽视的影响。 为了避免不必要和重复的工作，必须能够共享和重用已经训练过的模型。

加载一个已经训练过的 Transformer 模型很简单——我们可以使用 from_pretrained 方法来做到这一点：

```py
from transformers import BertModel

model = BertModel.from_pretrained("bert-base-cased")
```

正如您之前看到的，我们可以用等效的 AutoModel 类替换 BertModel。 我们将从现在开始这样做，因为这会产生与检查点无关的代码； 如果您的代码适用于一个检查点，它应该可以与另一个无缝协作。 即使架构不同，只要检查点是针对类似任务（例如，情感分析任务）训练的，这也适用。

在上面的代码示例中，我们没有使用 BertConfig，而是通过 bert-base-cased 标识符加载了一个预训练模型。 这是一个模型检查点，由 BERT 的作者自己训练； 您可以在[model card](https://huggingface.co/bert-base-cased).中找到有关它的更多详细信息。

该模型现在已使用检查点的所有权重进行初始化。 它可以直接用于对训练过的任务进行推理，也可以在新任务上进行微调。 通过使用预先训练好的权重进行训练，而不是从头开始，我们可以快速取得良好的效果。

 权重已下载并缓存在缓存文件夹中（因此以后调用 from_pretrained 方法不会重新下载它们），默认为 ~/.cache/huggingface/transformers。 您可以通过设置 HF_HOME 环境变量来自定义缓存文件夹。

 用于加载模型的标识符可以是 Model Hub 上任何模型的标识符，只要它与 BERT 架构兼容即可。 可以在[here](https://huggingface.co/models?filter=bert).找到可用 BERT 检查点的完整列表。 

# 保存模型

保存模型就像加载模型一样简单——我们使用 save_pretrained 方法，它类似于 from_pretrained 方法：

```python
model.save_pretrained("directory_on_my_computer")
```

这将保存两个文件到你的磁盘上：

```python
ls directory_on_my_computer

config.json pytorch_model.bin
```

如果您查看 config.json 文件，您将认识到构建模型架构所需的属性。 该文件还包含一些元数据，例如检查点的来源以及您上次保存检查点时使用的 🤗 Transformers 版本。

pytorch_model.bin 文件被称为状态字典； 它包含您模型的所有权重。 这两个文件齐头并进； 配置是了解模型架构所必需的，而模型权重是模型的参数。

# 使用一个Transformer模型进行推断

现在您知道如何加载和保存模型，让我们尝试使用它进行一些预测。 Transformer 模型只能处理数字——分词器生成的数字。 但在我们讨论分词器之前，让我们探讨一下模型接受哪些输入。

分词器可以负责将输入转换为适当框架的张量，但为了帮助您了解发生了什么，我们将快速了解在将输入发送到模型之前必须完成的操作。

假设我们有几个序列：

```python
sequences = [
  "Hello!",
  "Cool.",
  "Nice!"
]
```

分词器将这些转换为词汇索引，通常称为输入 ID。 每个序列现在都是一个数字列表！ 结果输出是：

```python
encoded_sequences = [
  [ 101, 7592,  999,  102],
  [ 101, 4658, 1012,  102],
  [ 101, 3835,  999,  102]
]
```

这是一个编码序列列表：一个包含列表的列表。 张量只接受矩形（想想矩阵）。 这个“数组”已经是矩形的，所以将它转换为张量很容易：

```python
import torch

model_inputs = torch.tensor(encoded_sequences)
```

# 使用张量作为模型的输入

在模型中使用张量非常简单——我们只用输入调用模型：

```python
output = model(model_inputs)
```

虽然模型接受许多不同的参数，但只需要输入 ID。 我们稍后将解释其他参数的作用以及何时需要它们，但首先我们需要仔细研究构建 Transformer 模型可以理解的输入的分词器。