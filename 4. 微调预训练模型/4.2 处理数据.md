# 处理数据

继续上一章的例子，这里是我们如何在 PyTorch 中训练一个批次的序列分类器： 

```python
import torch
from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification

# Same as before
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
    "I've been waiting for a HuggingFace course my whole life.",
    "This course is amazing!",
]
batch = tokenizer(sequences, padding=True, truncation=True, return_tensors="pt")

# This is new
batch["labels"] = torch.tensor([1, 1])

optimizer = AdamW(model.parameters())
loss = model(**batch).loss
loss.backward()
optimizer.step()
```

当然，仅仅在两个句子上训练模型不会产生很好的结果。 为了获得更好的结果，您需要准备更大的数据集。 

在本节中，我们将使用 William B. Dolan 和 Chris Brockett 在一篇论文中介绍的 MRPC（微软研究释义语料库）数据集作为示例。 该数据集由 5,801 对句子组成，带有一个标签，表明它们是否是释义（即，两个句子的意思是否相同）。 我们在本章中选择它是因为它是一个小数据集，因此很容易对其进行训练。 

# 从Hub中加载数据集

Hub 不仅包含模型； 它还有许多不同语言的多个数据集。 您可以在[here](https://huggingface.co/datasets)浏览数据集，我们建议您在完成本节后尝试加载和处理新数据集（请参阅[here](https://huggingface.co/docs/datasets/loading_datasets.html#from-the-huggingface-hub)的一般文档）。 但是现在，让我们专注于 MRPC 数据集！ 这是构成 GLUE 基准的 10 个数据集之一，该基准是一种学术基准，用于衡量 ML 模型在 10 个不同文本分类任务中的性能。 

🤗 数据集库提供了一个非常简单的命令来下载和缓存Hub上的数据集。 我们可以像这样下载 MRPC 数据集： 

```python
from datasets import load_dataset

raw_datasets = load_dataset("glue", "mrpc")
raw_datasets
DatasetDict({
    train: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 3668
    })
    validation: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 408
    })
    test: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 1725
    })
})
```

如您所见，我们得到一个 DatasetDict 对象，其中包含训练集、验证集和测试集。 每一个都包含几列（sentence1、sentence2、label 和 idx）和可变数量的行，它们是每个集合中元素的数量（因此，训练集中有 3,668 对句子，验证集中有 408 对 集，测试集中有 1,725 个）。 

此命令下载并缓存数据集，默认在 ~/.cache/huggingface/dataset 中。 回忆第 3 章，您可以通过设置 HF_HOME 环境变量来自定义缓存文件夹。

我们可以通过索引来访问 raw_datasets 对象中的每对句子，就像使用字典一样： 

```python
raw_train_dataset = raw_datasets["train"]
raw_train_dataset[0]
{'idx': 0,
 'label': 1,
 'sentence1': 'Amrozi accused his brother , whom he called " the witness " , of deliberately distorting his evidence .',
 'sentence2': 'Referring to him as only " the witness " , Amrozi accused his brother of deliberate
```

我们可以看到标签已经是整数，所以我们不必在那里做任何预处理。 要知道哪个整数对应哪个标签，我们可以检查 raw_train_dataset 的特征。 这将告诉我们每一列的类型：

```python
raw_train_dataset.features
{'sentence1': Value(dtype='string', id=None),
 'sentence2': Value(dtype='string', id=None),
 'label': ClassLabel(num_classes=2, names=['not_equivalent', 'equivalent'], names_file=None, id=None),
 'idx': Value(dtype='int32', id=None)}
```

在幕后，标签是 ClassLabel 类型，整数到标签名称的映射存储在名称文件夹中。 0 对应 not_equivalent，1 对应equivalent。 

