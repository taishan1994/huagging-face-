# 使用Trainer API来微调模型

🤗 Transformers 提供了一个 Trainer 类来帮助你在你的数据集上微调它提供的任何预训练模型。 完成上一节中的所有数据预处理工作后，您只需要执行几个步骤来定义训练器。 最困难的部分可能是准备运行 Trainer.train 的环境，因为它在 CPU 上运行速度非常慢。 如果您没有设置 GPU，则可以在 Google Colab 上访问免费的 GPU 或 TPU。 

下面的代码示例假定您已经执行了上一节中的示例。 这是一个简短的概括了您需要的摘要，： 

```python
from datasets import load_dataset
from transformers import AutoTokenizer, DataCollatorWithPadding

raw_datasets = load_dataset("glue", "mrpc")
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)

tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```

# 训练

定义 Trainer 之前的第一步是定义一个 TrainingArguments 类，该类将包含 Trainer 用于训练和评估的所有超参数。 您必须提供的唯一参数是保存训练模型的目录，以及沿途的检查点。 对于其余所有内容，您可以保留默认值，这对于基本微调应该非常有效。

```python
from transformers import TrainingArguments

training_args = TrainingArguments("test-trainer")
```

第二步是定义我们的模型。 和上一章一样，我们将使用 AutoModelForSequenceClassification 类，带有两个标签： 

```python
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
```

您会注意到，与第 3 章不同的是，在实例化此预训练模型后会收到警告。 这是因为 BERT 没有在句子对分类方面进行过预训练，所以预训练模型的头部已经被丢弃，而是添加了一个适合序列分类的新头部。 警告表明一些权重没有使用（对应于丢弃的预训练头的那些），而其他一些权重被随机初始化（新头的那些）。 最后鼓励您训练模型，这正是我们现在要做的。 一旦我们有了我们的模型，我们就可以定义一个训练器，将迄今为止构建的所有对象传递给它——模型、training_args、训练和验证数据集、我们的 data_collator 和我们的标记器： 

```python
from transformers import Trainer

trainer = Trainer(
    model,
    training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
)
```

请注意，当您像我们在此处所做的那样传递标记器时，Trainer 使用的默认 data_collator 将是之前定义的 DataCollatorWithPadding，因此您可以跳过此调用中的 data_collator=data_collator 行。 在第 3 节中向您展示这部分处理仍然很重要！ 

要在我们的数据集上微调模型，我们只需要调用 Trainer 的 train 方法： 

```py
trainer.train()
```

这将开始微调（在 GPU 上需要几分钟）并每 500 步报告一次训练损失。 但是，它不会告诉您模型的表现如何（或差）。 这是因为： 

- 我们没有通过将evaluation_strategy 设置为“steps”（评估每个eval_steps）或“epoch”（在每个epoch 结束时评估）来告诉Trainer 在训练期间进行评估。 
- 我们没有为 Trainer 提供一个 compute_metrics 函数来在所述评估期间计算指标（否则评估只会打印损失，这不是一个非常直观的数字）。 

# 验证

让我们看看如何构建有用的 compute_metrics 函数并在下次训练时使用它。 该函数必须采用一个 EvalPrediction 对象（它是一个具有预测字段和 label_ids 字段的命名元组），并将返回一个映射字符串到浮点数的字典（字符串是返回的度量的名称，而浮点数是它们的值）。 要从我们的模型中获得一些预测，我们可以使用 Trainer.predict 命令： 

```python
predictions = trainer.predict(tokenized_datasets["validation"])
print(predictions.predictions.shape, predictions.label_ids.shape)
(408, 2) (408,)
```

predict 方法的输出是另一个具有三个字段的命名元组：predictions、label_ids 和 metrics。 指标字段将只包含传递的数据集的损失，以及一些时间指标（预测所需的总时间和平均时间）。 一旦我们完成了我们的compute_metrics 函数并将其传递给Trainer，该字段也将包含compute_metrics 返回的指标。 

如您所见，预测是一个二维数组，形状为 408 x 2（408 是我们使用的数据集中元素的数量）。 这些是我们传递给预测的数据集的每个元素的对数（如您在前一章中看到的，所有 Transformer 模型都返回对数）。 要将它们转换为我们可以与标签进行比较的预测，我们需要在第二个轴上取最大值的索引： 

```python
import numpy as np
preds = np.argmax(predictions.predictions, axis=-1)
```

我们现在可以将这些预测与标签进行比较。 为了构建我们的 compute_metric 函数，我们将依赖 🤗 Datasets 库中的指标。 我们可以像加载数据集一样轻松加载与 MRPC 数据集关联的指标，这次是使用 load_metric 函数。 返回的对象有一个计算方法，我们可以用来进行度量计算： 

```python
from datasets import load_metric

metric = load_metric("glue", "mrpc")
metric.compute(predictions=preds, references=predictions.label_ids)
{'accuracy': 0.8578431372549019, 'f1': 0.8996539792387542}
```

您获得的确切结果可能会有所不同，因为模型头的随机初始化可能会改变它实现的指标。 在这里，我们可以看到我们的模型在验证集上的准确率为 85.78%，F1 分数为 89.97。 这是用于评估 GLUE 基准的 MRPC 数据集结果的两个指标。 BERT 论文中的表格报告了基本模型的 F1 分数为 88.9。 那是uncased模型，而我们目前使用的是cased模型，这说明了更好的结果。 

将所有内容包装在一起，我们得到了我们的 compute_metrics 函数： 

```python
def compute_metrics(eval_preds):
    metric = load_metric("glue", "mrpc")
    logits, labels = eval_preds
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)
```

为了查看在每个step结束时报告指标的实际应用，下面是我们如何使用这个计算指标功能定义一个新的Trainer：

```python
training_args = TrainingArguments("test-trainer", evaluation_strategy="epoch")
model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)

trainer = Trainer(
    model,
    training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)
```

请注意，我们创建了一个新的 TrainingArguments，其evaluation_strategy 设置为“epoch”和一个新模型——否则，我们只会继续训练我们已经训练过的模型。 要启动新的训练运行，我们执行： 

```python
trainer.train()
```

这一次，它将在训练损失之上报告每个 epoch 结束时的验证损失和指标。 同样，由于模型的随机头部初始化，您达到的准确准确率/F1 分数可能与我们发现的略有不同，但它应该在同一范围内。 

Trainer 将在多个 GPU 或 TPU 上开箱即用，并提供许多选项，例如混合精度训练（在训练参数中使用 fp16 = True）。 我们将以后讨论它支持的所有内容。 使用 Trainer API 进行微调的介绍到此结束。 第 7 章将给出一个对最常见的 NLP 任务执行此操作的示例，但现在让我们看看如何在纯 PyTorch 中执行相同的操作。 