# 章末测验

本章涵盖了很多内容！ 如果您没有掌握所有细节，请不要担心； 下一章将帮助您了解事物的内部运作方式。

不过，首先，让我们测试一下您在本章中学到的内容！

1. 探索 Hub 并寻找 roberta-large-mnli 检查点。 它执行什么任务？

- 总结
- **文字分类**
- 文本生成

更准确地说，它将两个句子划分为三个标签(矛盾，中性，蕴涵)，这也被称为自然语言推理。

2. 以下代码将返回什么？

```python
from transformers import pipeline

ner = pipeline("ner", grouped_entities=True)
ner("My name is Sylvain and I work at Hugging Face in Brooklyn.")
```

- 它将返回这个句子的分类分数，标签为“positive”或“negative”。
- 它将返回完成这句话的生成文本。
- **它将返回代表人、组织或地点的词。**

>  此外，使用 grouped_entities=True，它将属于同一实体的单词组合在一起，例如“Hugging Face”。

3. 在这个代码示例中，应该用什么来代替……？

```python
from transformers import pipeline

filler = pipeline("fill-mask", model="bert-base-cased")
result = filler("...")
```

- This has been waiting for you.
-  **This [MASK] has been waiting for you.**
-  This man has been waiting for you.

4. 为什么这段代码会失败？

```python
from transformers import pipeline

classifier = pipeline("zero-shot-classification")
result = classifier("This is a course about the Transformers library")
```

- 该管道要求给出标签来对文本进行分类。
- 这个管道需要几个句子，而不是一个。
- 🤗 Transformers像往常一样坏了。
- 这个管道需要更长的输入； 这个太短了。

> 正确的代码需要包含Candidate_labels=[...]。

5. “迁移学习”是什么意思？

- 通过在同一数据集上训练，将预训练模型的知识转移到新模型中。
- **通过使用第一个模型的权重初始化第二个模型，将预训练模型的知识转移到新模型。**
- 通过构建与第一个模型具有相同架构的第二个模型，将预训练模型的知识转移到新模型中。

> 当第二个模型接受新任务训练时，它迁移第一个模型的知识。

6. 对或错？ 语言模型的预训练通常不需要标签。

- **对**
- 错

> 预训练通常是自我监督的，这意味着标签是根据输入自动创建的（例如预测下一个单词或填充一些掩码单词）。

7. 选择最能描述术语“模型”、“架构”和“权重”的句子。

- 如果模型是建筑物，那么它的架构就是蓝图，权重就是住在里面的人。
- 架构是构建模型的地图，其权重是地图上表示的城市。
- **架构是用于构建模型的一系列数学函数，其权重是这些函数**

> 通过使用不同的参数（权重），可以使用相同的一组数学函数（架构）来构建不同的模型。

8. 您会使用以下哪种类型的模型来完成带有生成文本的提示？

- 编码器模型
- **解码器模型**
- 序列到序列模型

> 解码器模型非常适合根据提示生成文本。

9. 你会使用哪些类型的模型来总结文本？

- 编码器模型
- 解码器模型
- **序列到序列模型**

> 序列到序列模型非常适合摘要任务。

10. 您会使用以下哪种模型根据特定标签对文本输入进行分类？

- **编码器模型**
- 解码器模型
- 序列到序列模型

> 编码器模型生成整个句子的表示，非常适合分类等任务。

11. 在模型中观察到的偏差有哪些可能的来源？

- **该模型是预训练模型的微调版本，并从中获取了偏差。**
- **模型训练的数据是有偏差的。**
- **模型优化的指标存在偏差。**

> 一个不太明显的偏差来源是模型的训练方式。 您的模型将盲目地针对您选择的任何指标进行优化，不会有任何第二个想法。