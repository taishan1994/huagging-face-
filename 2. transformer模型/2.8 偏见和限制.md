# 偏见和限制

如果您打算在生产中使用预训练模型或微调版本，请注意，虽然这些模型是强大的工具，但它们也有局限性。 其中最重要的是，为了能够对大量数据进行预训练，研究人员通常会抓取他们能找到的所有内容，无论是最好的还是最坏的。

为了快速说明，让我们回到带有 BERT 模型的填充掩码管道的示例：

```python
from transformers import pipeline

unmasker = pipeline("fill-mask", model="bert-base-uncased")
result = unmasker("This man works as a [MASK].")
print([r["token_str"] for r in result])

result = unmasker("This woman works as a [MASK].")
print([r["token_str"] for r in result])

['lawyer', 'carpenter', 'doctor', 'waiter', 'mechanic']
['nurse', 'waitress', 'teacher', 'maid', 'prostitute']
```

当要求填写这两句话中缺失的单词时，模型只给出了一个不分性别的答案（服务员/女服务员）。 其他的是通常与一种特定性别相关的工作职业——是的，妓女最终出现在模型与“女人”和“工作”相关联的前 5 种可能性中。 尽管 BERT 是罕见的 Transformer 模型之一，但不是通过从互联网上抓取数据构建的，而是使用明显中性的数据（它在英语维基百科和 BookCorpus 数据集上训练），这种情况也会发生。

因此，当您使用这些工具时，您需要牢记您所使用的原始模型很容易产生性别歧视、种族主义或恐同内容。 根据您的数据对模型进行微调不会使这种内在偏差消失。