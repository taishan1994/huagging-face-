# 序列到序列模型

编码器-解码器模型（也称为序列到序列模型）使用 Transformer 架构的两个部分。 在每个阶段，编码器的注意力层可以访问初始句子中的所有单词，而解码器的注意力层只能访问位于输入中给定单词之前的单词。

这些模型的预训练可以使用编码器或解码器模型的目标来完成，但通常涉及更复杂的事情。 例如，T5 是通过用单个掩码特殊词替换随机文本跨度（可以包含多个单词）来预训练的，然后目标是预测该掩码替换的文本。

序列到序列模型最适合围绕根据给定输入生成新句子的任务，例如摘要、翻译或生成式问答。

该系列模型的代表包括：

- [BART](https://huggingface.co/transformers/model_doc/bart.html)
- [mBART](https://huggingface.co/transformers/model_doc/mbart.html)
- [Marian](https://huggingface.co/transformers/model_doc/marian.html)
- [T5](https://huggingface.co/transformers/model_doc/t5.html)